# -*- coding: UTF-8 -*-
"""
*@ description: è¯­ä¹‰åˆ†å‰² è®­ç»ƒ
*@ name:	train.py
*@ author: dengbozfan
*@ time:	2025/04/17 10:30
"""

from .. import nn,torch,os,tqdm,ic,csv
from .. import DataLoader, Optimizer, _LRScheduler
from .. import( 
    Accumulator,SegmentationMetric,
    evaluate_segmentation_loss,evaluate_segmentation_iou
)
from .. import PlotResult
from typing import Optional
from datetime import datetime

class Train:
    """è®­ç»ƒç±», ç”¨äºç®¡ç†æ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹"""
    def __init__(self,
                 model: nn.Module,
                 num_classes: int,
                 train_loader: DataLoader,
                 test_loader: DataLoader,
                 val_loader: Optional[DataLoader] = None,
                 run_dir : Optional[str] = None,
                 init_type = 'xavier',
                 device: Optional[str] = None
                 ):
        """
        åˆå§‹åŒ– Trainç±»
        Args:
            model (nn.Module) : å®ä¾‹åŒ–åçš„ç½‘ç»œæ¨¡å‹
            num_classes (int) : åˆ†ç±»æ•°ç›®
            train_loader (DataLoader): è®­ç»ƒé›†åŠ è½½å™¨
            test_loader (DataLoader): æµ‹è¯•é›†åŠ è½½å™¨
            val_loader (Optional[DataLoader]): éªŒè¯é›†æ•°æ®åŠ è½½å™¨. é»˜è®¤ä¸ºNone
            run_dir (Optional[str]): è®­ç»ƒä¿¡æ¯å­˜å‚¨æ–‡ä»¶ç›®å½• >> æœ€ä½³æ¨¡å‹ | æ£€æŸ¥ç‚¹ | è®­ç»ƒæŒ‡æ ‡
            init_type (str): æ¨¡å‹å‚æ•°åˆå§‹åŒ–ç±»å‹. é»˜è®¤ä¸º 'xavier'
        """
        self.model = model
        self.num_classes = num_classes
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.val_loader = val_loader
        self.run_dir = run_dir
        self.init_type = init_type

        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # æ£€æŸ¥ç‚¹ 
        self.best_test_iou = 0.0
        self.best_test_loss = float('inf')
        # åˆå§‹åŒ– run_dir
        self._initialize_run_dir()
        # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
        self._init_weights()

    def _initialize_run_dir(self):
        """åˆå§‹åŒ– run_dir, ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶åˆ›å»ºå¿…è¦çš„å­ç›®å½•"""

        if self.run_dir is None:
            # å¦‚æœ run_dir æœªæä¾›ï¼Œåˆ™åŠ¨æ€ç”Ÿæˆä¸€ä¸ªé»˜è®¤è·¯å¾„
            self.run_dir = os.path.join(os.getcwd(), "runs")

        self.run_count = self._get_run_count()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.run_subdir = os.path.join(self.run_dir, f"run{self.run_count + 1}_{timestamp}")
        ic(self.run_subdir)
        os.makedirs(self.run_subdir, exist_ok=True)

        # åˆ›å»ºå¿…è¦çš„å­ç›®å½•
        os.makedirs(os.path.join(self.run_subdir, "config"), exist_ok=True)
        os.makedirs(os.path.join(self.run_subdir, "model"), exist_ok=True)

        # è®¾ç½®æ¨¡å‹å­˜å‚¨è·¯å¾„
        self.best_model_path = os.path.join(self.run_subdir, "model", "best.pt")
        self.checkpoint_path = os.path.join(self.run_subdir, "model", "checkpoint.pt")
        self.train_log_path = os.path.join(self.run_subdir, "model", "trainLog.csv")

    def _get_run_count(self) -> int:
        """è·å–å½“å‰ run_path ä¸‹å·²æœ‰çš„ run æ•°é‡"""
        
        if not os.path.exists(self.run_dir):
            return 0
        run_dirs = [d for d in os.listdir(self.run_dir) if d.startswith("run")]
        return len(run_dirs)
    
    def _load_checkpoint(self,
                         checkpoint_path:str,
                         optimizer: Optimizer,
                         scheduler: Optional[_LRScheduler] = None) -> int:
        """
        ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨çš„çŠ¶æ€ã€‚

        Args:
            checkpoint_path (str): æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ã€‚

        Returns:
            int: å½“å‰ epochã€‚
        """
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint file not found: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if scheduler is not None and 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.best_test_iou = checkpoint.get('best_test_iou', 0.0)
        print(f"Model loaded from {checkpoint_path}")
        print(f"Best Validation Accuracy: {self.best_test_iou:.2f}%")

        return checkpoint['epoch']
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""

        for module in self.model.modules():
            if isinstance(module, nn.Conv2d):
                if self.init_type == 'kaiming':
                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                elif self.init_type == 'xavier':
                    nn.init.xavier_normal_(module.weight)
                else:
                    raise ValueError(f"Unsupported initialization type: {self.init_type}")
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm2d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Linear):
                if self.init_type == 'kaiming':
                    nn.init.kaiming_normal_(module.weight, nonlinearity='relu')
                elif self.init_type == 'xavier':
                    nn.init.xavier_normal_(module.weight)
                else:
                    raise ValueError(f"Unsupported initialization type: {self.init_type}")
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
        
        print(f"Model parameters initialized with {self.init_type} initialization.")
    
    def _train_net_epoch(self,
                         train_loader:DataLoader,
                         optimizer: Optimizer,
                         loss_func,
                         accum_iter:int = 1,
                         ):
        """è®­ç»ƒæ¨¡å‹ - one epoch"""
        """
        è®­ç»ƒå•æ‰¹æ¬¡ç½‘ç»œæ¨¡å‹
        Args:
            train_loader (DataLoader): è®­ç»ƒé›†åŠ è½½å™¨
            optimizer    (Optimizer) : æ¨¡å‹ä¼˜åŒ–å™¨
            loss_func    (function)  : æŸå¤±å‡½æ•°
            accum_iter   (int)       : ç´¯è®¡æ¢¯åº¦æ¬¡æ•°,æ¯ accum_iter ä¸ª batch æ›´æ–°ä¸€æ¬¡å‚æ•°
        """
        
        # åˆå§‹åŒ–æŒ‡æ ‡è®°å½•å™¨
        metric_log = Accumulator(4)
        metric = SegmentationMetric(4,self.device)
        with tqdm(train_loader, unit="batch") as train_bar:
            for i,(image, mask) in enumerate(train_bar):
                image,mask = image.to(self.device), mask.to(self.device)
                # å‰å‘ä¼ æ’­
                output = self.model(image)
                # è®¡ç®—æŸå¤±
                loss = loss_func(output,mask)
                # åå‘ä¼ æ’­
                loss.backward()
                if (i + 1) % accum_iter == 0: 
                    optimizer.step()
                    optimizer.zero_grad()

                with torch.no_grad():
                    output_class = torch.argmax(output, dim=1)
                    metric.addBatch(output_class, mask)
                    mean_iou = metric.meanIntersectionOverUnion()
                    metric_log.add(float(loss.mean()), len(image), mean_iou, 1)
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                train_bar.set_postfix(
                    batch_loss=float(loss.mean()),
                    avg_iou=metric_log[2]/metric_log[3]
                )
        # è®¡ç®—å¹³å‡æŸå¤±
        train_loss = metric_log[0] / metric_log[3]
        train_iou = metric_log[2] / metric_log[3]

        return train_loss, train_iou
    
    def train(self,
              num_epochs: int,
              optimizer: Optimizer,
              loss_func: nn.Module,
              scheduler: Optional[_LRScheduler] = None,
              save_checkpoint: bool = False,
              checkpoint_interval: int = 10,
              save_best_model: bool = True,
              resume_from_checkpoint:Optional[str] = None
              ):
        """
        è®­ç»ƒæ¨¡å‹
        Args:
            num_epochs(int): è®­ç»ƒçš„è½®æ•°
            optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨
            loss_func (nn.Module): æŸå¤±å‡½æ•°
            scheduler (Optional[_LRScheduler]): å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚é»˜è®¤ä¸º Noneã€‚
            save_checkpoint (bool): æ˜¯å¦ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚é»˜è®¤ä¸º Falseã€‚
            checkpoint_interval (int): ä¿å­˜æ£€æŸ¥ç‚¹çš„é—´éš”(å•ä½:epoch)ã€‚é»˜è®¤ä¸º 10ã€‚
            save_best_model (bool): æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚é»˜è®¤ä¸º Trueã€‚ 
            resume_from_checkpoint (Optional[str]): ä»æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤è®­ç»ƒã€‚é»˜è®¤ä¸º Noneã€‚ 
          
        """
        start_epoch = 0

        # å¦‚æœæä¾›äº†æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œåˆ™ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
        if resume_from_checkpoint is not None:
            start_epoch = self._load_checkpoint(resume_from_checkpoint, optimizer, scheduler)
            print(f"Resuming training from epoch {start_epoch + 1}")
        
        print(f"å¼€å§‹åœ¨{self.device} ä¸Šè¿›è¡Œè®­ç»ƒ")
        self.model.to(self.device)

        for epoch in range(start_epoch, num_epochs):
            # è®¡ç®—ä¸€æ¬¡è®­ç»ƒåçš„losså’Œiou
            train_loss, train_iou = self._train_net_epoch(self.train_loader,optimizer,loss_func,accum_iter=1)
            # è®¡ç®—å½“å‰çš„ test/val loss å’Œ test iou
            self.model.eval()
            # è®¡ç®—éªŒè¯é›†çš„æŸå¤±
            if self.val_loader is not None:
                val_loss = evaluate_segmentation_loss(self.model, self.val_loader, loss_func, self.device)
            else:
                val_loss = evaluate_segmentation_loss(self.model, self.test_loader, loss_func, self.device)
            # è®¡ç®—æµ‹è¯•é›†çš„IoU
            test_iou = evaluate_segmentation_iou(self.model, self.test_loader, self.num_classes, self.device)
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            if scheduler is not None:
                scheduler.step(val_loss)
            # å­˜å‚¨è®­ç»ƒæ•°æ® | å­¦ä¹ æ›²çº¿
            self._save_train_learn_data(epoch,train_loss,train_iou,val_loss,test_iou,current_lr)
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if save_best_model:
                self._save_best_model(self.best_model_path,test_iou)
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if save_checkpoint and (epoch + 1) % checkpoint_interval == 0:
                self._save_checkpoint(self.checkpoint_path,epoch+1,optimizer,scheduler)
            # æ¯ 2 ä¸ªepochç»˜åˆ¶ä¸€æ¬¡è®­ç»ƒæ›²çº¿
            if (epoch + 1) % 2 == 0:
                self.save_train_data(epoch)

        # æœ€åä¿å­˜è®­ç»ƒæ•°æ®
        self.save_train_data(epoch)

        print("è®­ç»ƒå®Œæˆäº†ï¼ğŸ˜ŠğŸ‰ãƒ¾(â‰§â–½â‰¦*)o")

    def save_train_data(self,epoch):
        """ä¿å­˜è®­ç»ƒç»“æœ"""

        self.trainResult_save_dir = f'{self.run_subdir}\config\config_{epoch+1}'

        os.makedirs(self.trainResult_save_dir,exist_ok=True)
        plot = PlotResult(train_log_path=self.train_log_path,save_dir=self.trainResult_save_dir)

        plot.plotAll()
        plot.compute_and_plot_confusion_matrix(epoch,self.num_classes,self.model,self.test_loader,self.device)

    def _save_train_learn_data(self,epoch:int, train_loss:float, train_iou:float,val_loss:float,test_iou:float,current_lr:float):
        """
        å­˜å‚¨ è®­ç»ƒå­¦ä¹ æ•°æ® | ç»˜åˆ¶å­¦ä¹ æ›²çº¿
        Args:
            epoch       (int)    : å½“å‰è®­ç»ƒæ‰¹æ¬¡
            train_loss  (float)  : è®­ç»ƒæŸå¤±
            train_iou   (float)  : è®­ç»ƒIou
            val_loss    (float)  : éªŒè¯æŸå¤±
            test_iou    (float)  : æµ‹è¯•iou
            current_lr  (float)  : å½“å‰å­¦ä¹ ç‡
        """
        with open(self.train_log_path, mode='a', newline='') as file:
            writer = csv.writer(file)
            if epoch == 0:
                writer.writerow(['Epoch', 'Train Loss', 'Train Acc', 'Val Loss', 'Test Acc', 'LR'])
            
            writer.writerow([epoch, f"{train_loss:.5f}", f"{train_iou:.5f}", f"{val_loss:.5f}", f"{test_iou:.3f}",f'{current_lr:.6f}'])
        
        print(f"Epoch {epoch}, Train Loss: {train_loss:.5f}, Train Acc: {train_iou:.5f}, Val Loss: {val_loss:.5f}, Test Acc: {test_iou:.5f}, LR: {current_lr:.5f}")
            

    def _save_best_model(self,path:str, test_iou: float):
        """ 
        ä¿å­˜æœ€ä½³æ¨¡å‹
        Args:
            path (str): æœ€ä½³æ¨¡å‹å­˜å‚¨è·¯å¾„
            test_iou (float) : å½“å‰è®­ç»ƒåè®¡ç®—å¾—å‡ºçš„iou
        """
        if test_iou > self.best_test_iou:
            self.best_test_iou = test_iou
            torch.save(self.model.state_dict(), path)
            print(f'å·²ä¿å­˜å½“å‰æœ€ä½³æ¨¡å‹, Best Test IoU: {self.best_test_iou:.4f}')
        

    def _save_checkpoint(self, path: str, epoch: int, optimizer: Optimizer, scheduler: Optional[_LRScheduler]):
        """
        ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚

        Args:
            path (str): æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„ã€‚
            epoch (int): å½“å‰ epochã€‚
            optimizer (Optimizer): ä¼˜åŒ–å™¨ã€‚
            scheduler (Optional[_LRScheduler]): å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
        """
        if self.run_dir is not None:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,
                'best_test_loss': self.best_test_loss,
                'best_test_iou': self.best_test_iou
            }
            torch.save(checkpoint, path)
            print(f"Checkpoint saved to {path}")
